{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"device: \", device)\n",
    "torch.device(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CFar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NNModel (nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 12, 3, padding='same')\n",
    "        self.batch1 = nn.BatchNorm2d(12)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(12, 32, 3, padding='same')\n",
    "        self.batch2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding='same')\n",
    "        self.batch3 = nn.BatchNorm2d(64)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "        self.dropout = nn.Dropout(0.10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batch2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.batch3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.flatten(x)\n",
    "        #x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manuelrodriguez/miniconda3/envs/pytorch2/lib/python3.8/site-packages/torchinfo/torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  action_fn=lambda data: sys.getsizeof(data.storage()),\n",
      "/Users/manuelrodriguez/miniconda3/envs/pytorch2/lib/python3.8/site-packages/torch/storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return super().__sizeof__() + self.nbytes()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #\n",
       "===================================================================================================================\n",
       "NNModel                                  [64, 3, 32, 32]           [64, 10]                  --\n",
       "├─Conv2d: 1-1                            [64, 3, 32, 32]           [64, 12, 32, 32]          336\n",
       "├─BatchNorm2d: 1-2                       [64, 12, 32, 32]          [64, 12, 32, 32]          24\n",
       "├─ReLU: 1-3                              [64, 12, 32, 32]          [64, 12, 32, 32]          --\n",
       "├─MaxPool2d: 1-4                         [64, 12, 32, 32]          [64, 12, 16, 16]          --\n",
       "├─Conv2d: 1-5                            [64, 12, 16, 16]          [64, 32, 16, 16]          3,488\n",
       "├─BatchNorm2d: 1-6                       [64, 32, 16, 16]          [64, 32, 16, 16]          64\n",
       "├─ReLU: 1-7                              [64, 32, 16, 16]          [64, 32, 16, 16]          --\n",
       "├─MaxPool2d: 1-8                         [64, 32, 16, 16]          [64, 32, 8, 8]            --\n",
       "├─Conv2d: 1-9                            [64, 32, 8, 8]            [64, 64, 8, 8]            18,496\n",
       "├─BatchNorm2d: 1-10                      [64, 64, 8, 8]            [64, 64, 8, 8]            128\n",
       "├─ReLU: 1-11                             [64, 64, 8, 8]            [64, 64, 8, 8]            --\n",
       "├─MaxPool2d: 1-12                        [64, 64, 8, 8]            [64, 64, 4, 4]            --\n",
       "├─Flatten: 1-13                          [64, 64, 4, 4]            [64, 1024]                --\n",
       "├─Dropout: 1-14                          [64, 1024]                [64, 1024]                --\n",
       "├─Linear: 1-15                           [64, 1024]                [64, 512]                 524,800\n",
       "├─ReLU: 1-16                             [64, 512]                 [64, 512]                 --\n",
       "├─Linear: 1-17                           [64, 512]                 [64, 128]                 65,664\n",
       "├─ReLU: 1-18                             [64, 128]                 [64, 128]                 --\n",
       "├─Linear: 1-19                           [64, 128]                 [64, 10]                  1,290\n",
       "├─ReLU: 1-20                             [64, 10]                  [64, 10]                  --\n",
       "===================================================================================================================\n",
       "Total params: 614,290\n",
       "Trainable params: 614,290\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 192.81\n",
       "===================================================================================================================\n",
       "Input size (MB): 0.79\n",
       "Forward/backward pass size (MB): 25.50\n",
       "Params size (MB): 2.46\n",
       "Estimated Total Size (MB): 28.74\n",
       "==================================================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "model = NNModel(10)\n",
    "summary(model, input_size=(64, 3, 32, 32), device='cpu', col_names=['input_size', 'output_size',\n",
    "                                                                           'num_params'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "lost_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "--------------\n",
      "loss: 2.305736  [    0/50000]\n",
      "loss: 2.097169  [ 6400/50000]\n",
      "loss: 2.220761  [12800/50000]\n",
      "loss: 2.165665  [19200/50000]\n",
      "loss: 2.013907  [25600/50000]\n",
      "loss: 1.776513  [32000/50000]\n",
      "loss: 1.959481  [38400/50000]\n",
      "loss: 1.861729  [44800/50000]\n",
      "Test Error: \n",
      " Accuracy: 34.2%, Avg loss: 1.864391 \n",
      "\n",
      "Epoch:  1\n",
      "--------------\n",
      "loss: 1.727889  [    0/50000]\n",
      "loss: 1.784922  [ 6400/50000]\n",
      "loss: 1.724765  [12800/50000]\n",
      "loss: 1.922395  [19200/50000]\n",
      "loss: 1.913162  [25600/50000]\n",
      "loss: 1.888348  [32000/50000]\n",
      "loss: 1.912720  [38400/50000]\n",
      "loss: 1.892442  [44800/50000]\n",
      "Test Error: \n",
      " Accuracy: 36.6%, Avg loss: 1.763880 \n",
      "\n",
      "Epoch:  2\n",
      "--------------\n",
      "loss: 1.716307  [    0/50000]\n",
      "loss: 1.578296  [ 6400/50000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     16\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 17\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     18\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     21\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch2/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch2/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    print(\"Epoch: \", epoch)\n",
    "    print(\"--------------\") \n",
    "\n",
    "    running_loss = 0.0\n",
    "    size = len(trainloader.dataset)\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            loss, current = loss.item(), i * len(inputs)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        \n",
    "        # print statistics\n",
    "        #running_loss += loss.item()\n",
    "        #if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "        #    print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "        #    running_loss = 0.0\n",
    "        \n",
    "    size = len(testloader.dataset)\n",
    "    num_batches = len(testloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in testloader:\n",
    "            #X = X.to(device)\n",
    "            #y = y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += criterion(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
